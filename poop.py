# === 1. –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ ===
import pandas as pd
import numpy as np
from scipy.stats import kurtosis, zscore
from sklearn.cluster import DBSCAN
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# === 2. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
file_path = "VKR_dataset_test.xlsx"
df1 = pd.read_excel(file_path)
df1.rename(columns={df1.columns[0]: "timestamp"}, inplace=True)
df1["timestamp"] = pd.to_datetime(df1["timestamp"], format="%d.%m.%Y %H:%M:%S", errors="coerce")
df1["–î–∞—Ç–∞"] = df1["timestamp"].dt.date
df1["–í—Ä–µ–º—è"] = df1["timestamp"].dt.time
df1 = df1.drop(columns=["timestamp"])
columns_order = ["–î–∞—Ç–∞", "–í—Ä–µ–º—è"] + [col for col in df1.columns if col not in ["–î–∞—Ç–∞", "–í—Ä–µ–º—è"]]
df1 = df1[columns_order].dropna()
df1.to_csv("processed_data.csv", index=False)
print("\n‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'processed_data.csv'")

# === 3. –í—ã–±–æ—Ä –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ ===
df = pd.read_csv("processed_data.csv")
columns_to_analyze = [col for col in df.columns if col not in ["–î–∞—Ç–∞", "–í—Ä–µ–º—è"]]


# === 4. –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –ø–æ Z-score ===
def remove_outliers_zscore(df, threshold=3):
    z_scores = np.abs(zscore(df))
    mask = (z_scores < threshold).all(axis=1)
    return df[mask]


df_cleaned = remove_outliers_zscore(df[columns_to_analyze])
print("\n‚úÖ –î–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã –æ—Ç –≤—ã–±—Ä–æ—Å–æ–≤ –ø–æ Z-score")

# === 5. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é DBSCAN ===
dbscan = DBSCAN(eps=1.5, min_samples=5)
labels = dbscan.fit_predict(df_cleaned)
df_dbscan_cleaned = df_cleaned[labels != -1]
print("\n‚úÖ –®—É–º —É–¥–∞–ª—ë–Ω –º–µ—Ç–æ–¥–æ–º DBSCAN")

# === 6. –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ "–î–∞—Ç–∞" –∏ "–í—Ä–µ–º—è" ===
df_cleaned_full = df.loc[df_dbscan_cleaned.index, :]
df_cleaned_full.to_csv("cleaned_data.csv", index=False)
print("\n‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–µ –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'cleaned_data.csv'")


# === 7. –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ ===
def apply_moving_average(df, columns, window_size=3):
    smoothed_data = df.copy()
    for col in columns:
        smoothed_data[col] = df[col].rolling(window=window_size, center=True).mean()
    return smoothed_data.dropna()


df_smoothed = apply_moving_average(df_cleaned_full, columns_to_analyze)
df_smoothed.to_csv("smoothed_data.csv", index=False)
print("\n‚úÖ –î–∞–Ω–Ω—ã–µ —Å–≥–ª–∞–∂–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'smoothed_data.csv'")


# === 8. –û—Ü–µ–Ω–∫–∞ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è ===
def calculate_metrics(original, smoothed, column):
    aligned_original = original.loc[smoothed.index]
    rmse = np.sqrt(mean_squared_error(aligned_original[column], smoothed[column]))
    variance_reduction = (np.var(aligned_original[column]) - np.var(smoothed[column])) / np.var(
        aligned_original[column]) * 100
    print(f"\nüîç {column}: RMSE = {rmse:.4f}, –°–Ω–∏–∂–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ = {variance_reduction:.2f}%")


for col in columns_to_analyze:
    calculate_metrics(df_cleaned_full, df_smoothed, col)


# === 9. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ü–∏–ª–∏–Ω–¥—Ä–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ ===
# (1) –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–µ—Ä–∏–æ–¥–∞
# (2) –†–∞–∑–≤—ë—Ä—Ç–∫–∞ –≤ —Ü–∏–ª–∏–Ω–¥—Ä–∏—á–µ—Å–∫—É—é –º–æ–¥–µ–ª—å
# (3) –ú–µ—Ç–æ–¥ –Ω–∞–∏–º–µ–Ω—å—à–∏—Ö –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ (–ú–ù–ö)
# (4) –ü—Å–µ–≤–¥–æ–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

def estimate_period(series, min_period=2, max_period=100):
    errors = []
    for p in range(min_period, max_period + 1):
        segments = series[:len(series) // p * p].reshape(-1, p)
        mean_segment = segments.mean(axis=0)
        error = np.mean((segments - mean_segment) ** 2)
        errors.append((p, error))
    best_period, _ = min(errors, key=lambda x: x[1])
    return best_period


def build_cylindrical_model(series, period):
    n = len(series)
    num_cycles = n // period
    return series[:num_cycles * period].reshape(num_cycles, period)


def least_squares_fit(cyl_model):
    X = cyl_model[:-1]
    Y = cyl_model[1:]
    params = np.linalg.pinv(X) @ Y
    return params


def pseudo_gradient_update(X, Y, alpha=0.001, iterations=100):
    params = np.random.randn(X.shape[1], X.shape[1])
    for _ in range(iterations):
        grad = -2 * X.T @ (Y - X @ params) / len(X)
        params -= alpha * grad
    return params


for col in columns_to_analyze:
    print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–∞: {col}")
    series = df_smoothed[col].values
    period = estimate_period(series)
    print(f"üìè –û–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥ –¥–ª—è {col}: {period}")
    cyl_model = build_cylindrical_model(series, period)
    params_ls = least_squares_fit(cyl_model)
    params_pg = pseudo_gradient_update(cyl_model[:-1], cyl_model[1:])
    print(f"‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–∞ {col} –æ–±—É—á–µ–Ω—ã –º–µ—Ç–æ–¥–æ–º –ú–ù–ö –∏ –ø—Å–µ–≤–¥–æ–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º –º–µ—Ç–æ–¥–æ–º")
print("\n‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω—ã –º–µ—Ç–æ–¥–æ–º –ú–ù–ö –∏ –ø—Å–µ–≤–¥–æ–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º –º–µ—Ç–æ–¥–æ–º")


def plot_comparison(original, smoothed, column):
    plt.figure(figsize=(12, 6))
    plt.plot(original[column], label="Original Data", alpha=0.7)
    plt.plot(smoothed[column], label="Smoothed Data (Moving Average)", alpha=0.9)
    plt.title(f"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö - {column}")
    plt.xlabel("Index")
    plt.ylabel(column)
    plt.legend()
    plt.grid()
    plt.show()


# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
plot_comparison(df_cleaned, df_smoothed, "Motor_current")
