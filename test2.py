import pandas as pd
import numpy as np
from scipy.stats import kurtosis, zscore
from sklearn.impute import KNNImputer
from sklearn.cluster import DBSCAN

file_path = "VKR_dataset_test.xlsx"  # –ò–º—è —Ñ–∞–π–ª–∞ Excel
df1 = pd.read_excel(file_path)

if df1.shape[1] < 2:
    raise ValueError("–û—à–∏–±–∫–∞: –í —Ñ–∞–π–ª–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö!")
df1.rename(columns={df1.columns[0]: "timestamp"}, inplace=True)  # –ü–µ—Ä–µ–∏–º–µ–Ω—É–µ–º –ø–µ—Ä–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü
df1["timestamp"] = pd.to_datetime(df1["timestamp"], format="%d.%m.%Y %H:%M:%S", errors="coerce")
df1["–î–∞—Ç–∞"] = df1["timestamp"].dt.date
df1["–í—Ä–µ–º—è"] = df1["timestamp"].dt.time
df1 = df1.drop(columns=["timestamp"])  # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É
columns_order = ["–î–∞—Ç–∞", "–í—Ä–µ–º—è"] + [col for col in df1.columns if col not in ["–î–∞—Ç–∞", "–í—Ä–µ–º—è"]]
df1 = df1[columns_order]
df1 = df1.dropna()
print("\n –î–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–∏:")
print(df1.head())
df1.to_csv("processed_data.csv", index=False)
print("\n –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'processed_data.csv'")

df = pd.read_csv("processed_data.csv")
# === 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã ===
columns_to_analyze = ["TP2", "TP3", "H1", "DV_pressure", "Reservoirs", "Oil_temperature", "Motor_current"]
df_numeric = df[columns_to_analyze]


# === 3. Kurtosis Measure (–≠–∫—Å—Ü–µ—Å—Å) ===
def detect_outliers_kurtosis(df, threshold=3):
    kurt_values = df.apply(kurtosis)
    return kurt_values[abs(kurt_values) > threshold]


outliers_kurtosis = detect_outliers_kurtosis(df_numeric)
print("\nüîπ –í—ã–±—Ä–æ—Å—ã –ø–æ Kurtosis:\n", outliers_kurtosis)


# === 4. Z-score (–û—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ) ===
def detect_outliers_zscore(df, threshold=3):
    z_scores = np.abs(zscore(df))
    return np.where(z_scores > threshold)


outlier_rows, outlier_cols = detect_outliers_zscore(df_numeric)
print(f"\nüîπ –ù–∞–π–¥–µ–Ω–æ {len(set(outlier_rows))} —Å—Ç—Ä–æ–∫ —Å –≤—ã–±—Ä–æ—Å–∞–º–∏ –ø–æ Z-score.")

# === 5. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö: KNNImputer ===
imputer = KNNImputer(n_neighbors=5)
df_knn_cleaned = pd.DataFrame(imputer.fit_transform(df_numeric), columns=df_numeric.columns)
print("\n‚úÖ –î–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã —Å –ø–æ–º–æ—â—å—é KNNImputer.")

# === 6. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö: DBSCAN ===
dbscan = DBSCAN(eps=1.5, min_samples=5)
labels = dbscan.fit_predict(df_knn_cleaned)
df_dbscan_cleaned = df_knn_cleaned[labels != -1]  # –£–¥–∞–ª—è–µ–º —à—É–º–æ–≤—ã–µ —Ç–æ—á–∫–∏
print("\n‚úÖ –î–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã —Å –ø–æ–º–æ—â—å—é DBSCAN.")

# === 7. –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º "–î–∞—Ç–∞" –∏ "–í—Ä–µ–º—è" ===
df_cleaned = df.iloc[df_dbscan_cleaned.index, :]

# === 8. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ===
df_cleaned.to_csv("cleaned_data.csv", index=False)
print("\n‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–µ –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'cleaned_data.csv'.")


# === 9. –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–º —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ ===
def apply_moving_average(df, columns, window_size=3):
    smoothed_data = df.copy()
    for col in columns:
        smoothed_data[col] = df[col].rolling(window=window_size, center=True).mean()
    return smoothed_data


# –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
df_smoothed = apply_moving_average(df_cleaned, columns_to_analyze, window_size=3)
df_smoothed = df_smoothed.dropna()  # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –ø–æ—Å–ª–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è
df_smoothed.to_csv("smoothed_data.csv", index=False)
print("\n‚úÖ –î–∞–Ω–Ω—ã–µ —Å–≥–ª–∞–∂–µ–Ω—ã –º–µ—Ç–æ–¥–æ–º —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'smoothed_data.csv'.")

from sklearn.metrics import mean_squared_error


def calculate_metrics(original, smoothed, column):
    # –û–±—Ä–µ–∑–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ –¥–ª–∏–Ω—ã —Å–≥–ª–∞–∂–µ–Ω–Ω—ã—Ö
    aligned_original = original.loc[smoothed.index]

    # –í—ã—á–∏—Å–ª—è–µ–º MSE –∏ RMSE
    mse = mean_squared_error(aligned_original[column], smoothed[column])  # MSE
    rmse = np.sqrt(mse)  # –ö–æ—Ä–µ–Ω—å –∏–∑ MSE –¥–ª—è RMSE

    # –°–Ω–∏–∂–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏
    variance_reduction = (np.var(aligned_original[column]) - np.var(smoothed[column])) / np.var(
        aligned_original[column]) * 100

    print(f"RMSE –¥–ª—è {column}: {rmse}")
    print(f"–°–Ω–∏–∂–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –¥–ª—è {column}: {variance_reduction:.2f}%")


# –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ NaN —É–¥–∞–ª–µ–Ω—ã –ø–µ—Ä–µ–¥ —Ä–∞—Å—á–µ—Ç–æ–º –º–µ—Ç—Ä–∏–∫
df_smoothed = df_smoothed.dropna()  # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN
calculate_metrics(df_cleaned, df_smoothed, "TP2")

"""
def calculate_metrics_with_nan(original, smoothed, column):
    # RMSE –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç NaN
    rmse = np.sqrt(np.nanmean((original[column] - smoothed[column]) ** 2))

    # –°–Ω–∏–∂–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏
    variance_reduction = (np.nanvar(original[column]) - np.nanvar(smoothed[column])) / np.nanvar(original[column]) * 100

    print(f"RMSE –¥–ª—è {column} (—Å —É—á–µ—Ç–æ–º NaN): {rmse}")
    print(f"–°–Ω–∏–∂–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –¥–ª—è {column} (—Å —É—á–µ—Ç–æ–º NaN): {variance_reduction:.2f}%")


# –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –±–µ–∑ –æ–±—Ä–µ–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
calculate_metrics_with_nan(df_cleaned, df_smoothed, "TP2")
"""

import matplotlib.pyplot as plt


def plot_comparison(original, smoothed, column):
    plt.figure(figsize=(12, 6))
    plt.plot(original[column], label="Original Data", alpha=0.7)
    plt.plot(smoothed[column], label="Smoothed Data (Moving Average)", alpha=0.9)
    plt.title(f"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö - {column}")
    plt.xlabel("Index")
    plt.ylabel(column)
    plt.legend()
    plt.grid()
    plt.show()


# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
plot_comparison(df_cleaned, df_smoothed, "TP2")
